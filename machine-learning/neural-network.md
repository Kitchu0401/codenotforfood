# Chapter 3. 신경망

## 일반적인 신경망의 구조

신경망의 가장 기본적인 형태는 입력층 / 은닉층 / 출력층으로 구성됨.
- 입력층: 데이터가 신경망으로 입력되는 부분.
- 은닉층: 입력층 또는 이전 노드에서 전달 받은 데이터를 처리해 다음 노드 또는 출력층에 전달할 결과값으로 반환해주는 노드들이 위치한 층.
- 출력층: 신경망을 통해 처리된 학습의 결과를 확인 할 수 있는 부분, 특정 라벨 - 예를 들어 사진이 입력되었을 경우 사진의 물체가 사람인지 개인지 고양이인지 - 의 확률을 구할 수 있음. (logistic, softmax, etc..) 

## 퍼셉트론의 복습과 활성화 함수

퍼셉트론은 이전 노드들로부터 신호를 전달 받아 가중치를 곱해 총 합을 구한 뒤, 편향을 고려하여 해당 노드의 출력값을 결정하는 구조.

![노드의 구조](perceptron_node.png)

퍼셉트론은 위 그림처럼 입력값을 종합하는 노드와 이를 신호로 치환하는 **활성함수**로 나눌 수 있음.

## 활성화 함수 - 계단 함수 vs 시그노이드 함수 vs 렐루(ReLU) 함수

- 계단 함수: 입력값에 따라 0 또는 1을 출력값으로 반환하는 함수
- 시그노이드 함수: 입력값에 따라 0에서 1까지를 출력값으로 반환하는 함수
- 렐루 함수: 입력값이 0 이하일 경우 0을, 0을 초과할 경우 입력값을 반환하는 함수

공통점
- 비선형 함수
- 출력값이 0에서 1 사이
- 입력값이 작을수록 결과값이 0에 가까워지고, 입력값이 커질수록 결과값이 1에 가까워짐

차이점
- '매끄러움'의 차이
- 계단 함수는 0과 1만을, 시그노이드 함수는 연속된 실수를 결과값으로 반환

## 시그모이드 함수, 도대체 왜..?

![시그모이드 함수(로지스틱 회귀 함수)](logistic_regression.png)

- 로지스틱 회귀 함수라고도 함
- 은닉층을 다층으로 구성하는 이점이 없기 때문?
- 계단 함수에서의 선형 결과가 아닌 비선형 함수로 결과를 출력하기 위해서.
- 수식화 하더라도 범위가 방대한 데이터를 0과 1 사이의 값으로 바꾸어주기 위해
  - 입력값을 양수로 바꾸어주기 위해 e^(-x)가 들어감 - 어떤 값이 x에 들어오든 양수가 됨
  - x가 클수록 확률이 증가하도록 하기 위해 e^(-x)가 분모에 들어감
  - x가 커지면:
    - e^(-x)는 작아지고
    - 1 / 1 + e^(-x)는 1에 가까워짐
  - x가 작아지면:
    - e^(-x)는 커지고
    - 1 / 1 + e^(-x)는 0에 가까워짐

## 다차원 배열 계산 with Numpy

행렬의 내적 연산 기본
- 노드의 가중치 계산을 보다 편리하게 수행하기 위해 사용
- 행렬의 형상에 주의: 행렬 A의 열 수 must equal 행렬 B의 행 수 ([1, "2"] dot ["3", 4])
- 내적 연산 결과의 형상은 행렬 A의 행 수 x 행렬 B의 열 수가 됨

> a x b * c x d = a * d (when b == c)

## 신경망 순방향 구현

p85-89에서의 두 행렬 내적 연산을 통한 기본적인 신경망 구현을 들여다보자. [참고링크](http://blog.naver.com/PostView.nhn?blogId=htk1019&logNo=220980094465)

```py
import numpy as np

# 입력층 - 첫 번째 은닉층

X = np.array([1.0, 0.5])                          # 입력층으로부터의 신호 (1 x 2)
W = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])  # 입력층에서 첫 번째 은닉층으로의 간선 가중치 (2 x 3)
B = np.array([0.1, 0.2, 0.3])                     # 첫 번째 은닉층의 편향 (1 x 3)

# print절 생략

# 이전 노드들에서 가중치를 적용하여 합산된 값이 내적 연산으로 추출되고
# 이 때 내적 연산 결과의 형상은 1 x 3이 된다.
# 이후 브로드캐스팅을 통해 편향들이 각 내적값에 합산(+ B)된다.
A = np.dot(X, W) + B

print(A)
# 1.0 * 0.1 + 0.5 * 0.2 = 0.2 + (0.1) = 0.3
# 1.0 * 0.3 + 0.5 * 0.4 = 0.5 + (0.2) = 0.7
# 1.0 * 0.5 + 0.5 * 0.6 = 0.8 + (0.3) = 1.1
>>> [0.3, 0.7, 1.1]

# 시그모이드 함수 적용
Z = sigmoid(A)
>>> [0.57444252, 0.66818777, 0.75026011]

# 첫 번째 은닉층 - 두 번째 은닉층

W2 = np.array([[1.0, 0.4], [0.2, 0.5], [0.3, 0.5]]) # 첫 번째 은닉층에서 두 번째 은닉층으로의 간선 가중치 (3 x 2)
B2 = np.array([0.1, 0.2])                           # 두 번째 은닉층의 편향 (1 x 2)

A2 = np.dot(Z, W2) + B2       # 활성화 함수를 거치기 전의 입력 신호의 총 합
>>> [1.03315811, 1.13900095]  # 이 때 입력 신호의 총 합이 1 이상이 되는데
Z2 = sigmoid(A2)              
>>> [0.73755277, 0.75749616]  # 활성화 함수를 거치면서 0에서 1 사이의 값으로 조정된다.

# 출력층으로의 신호 전달도 동일하게
# 다만, 출력층에선 목적에 따라 활성화 함수를 다르게 적용한다. (항등 함수, 소프트맥스 함수 등)
```

## 출력층 설계 - 소프트맥스 함수

신경망의 목적 - 분류와 회기 - 에 따라 다른 활성화 함수를 출력층에 적용한다.
- 

# 덧.

## 델타규칙

퍼셉트론을 통해 출력되는 값과 실제 값을 비교하여 차이가 생길 경우, 그 차이를 퍼셉트론에 반영(feedback)하여 편향과 가중치를 조정, 더 나은 출력값을 가질 수 있도록 학습을 시키는 것.

## 활성화 함수 - 소프트맥스 함수

노드의 출력값을 0~1 사이의 값으로 정규화하며 출력값들의 총합이 항상 1이 되는 함수 [링크](https://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221021710286)

## 인공지능 구현의 이슈와 그 해결

너무 인간답지 못하게 학습한 나머지 모든 데이터를 외워버리는 Overfitting 문제
- Dropout
- Batch Normalization

학습하고 보니 우물 안 개구리가 되어서 빠져나가지 못하는 local minima 문제
- Backpropagation [링크](http://happycontrol.tistory.com/entry/%EC%97%AD%EC%A0%84%ED%8C%8C-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98BackPropagation-%EC%88%98%EC%97%85%EB%B2%84%EC%A0%84?category=482449)
- Pretraining: 데이터와 층을 미리 학습
